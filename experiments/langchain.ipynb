{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "from hugchat.login import Login\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "  n:int\n",
    "  \n",
    "  @property\n",
    "  def _llm_type(self) -> str:\n",
    "    return \"custom\"\n",
    "\n",
    "  def _call(self,prompt:str,stop:Optional[List[str]]=None) -> str:\n",
    "    if stop is not None:\n",
    "      raise ValueError(\"stop kwargs are not permitted\")\n",
    "    return prompt[:self.n]\n",
    "  \n",
    "  @property\n",
    "  def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'222'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = CustomLLM(n=111)\n",
    "llm(\"222\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugchat Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HuggingChat(LLM):\n",
    "    \n",
    "    history_data: Optional[List] = []\n",
    "    chatbot : Optional[hugchat.ChatBot] = None\n",
    "    conversation : Optional[str] = \"\"\n",
    "    email : Optional[str]\n",
    "    psw : Optional[str]\n",
    "    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            pass\n",
    "            #raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        #token is a must check\n",
    "        if self.chatbot is None:\n",
    "            if self.email is None and self.psw is None:\n",
    "                ValueError(\"Email and Password is required, pls check the documentation on github\")\n",
    "            else: \n",
    "                if self.conversation == \"\":\n",
    "                    sign = Login(self.email, self.psw)\n",
    "                    cookies = sign.login()\n",
    "\n",
    "                    # Save cookies to usercookies/<email>.json\n",
    "                    sign.saveCookiesToDir()\n",
    "\n",
    "                    # Create a ChatBot\n",
    "                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n",
    "                else:\n",
    "                    raise ValueError(\"Something went wrong\")\n",
    "            \n",
    "        \n",
    "        # sleep(5)\n",
    "        data = self.chatbot.query(prompt, temperature=0.5, stream=False)\n",
    "    \n",
    "        \n",
    "        #add to history\n",
    "        # self.history_data.append({\"prompt\":prompt,\"response\":data})    \n",
    "        # print(data)\n",
    "        # print(data.get_final_text())\n",
    "        # data.get_final_text()\n",
    "        return str(data)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model\": \"HuggingCHAT\"}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "email=os.environ[\"emailHF\"]\n",
    "passwd= os.environ[\"pswHF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingChat(email  , psw ) #for start new chat\n",
    "\n",
    "\n",
    "# print(llm(\"Hello, how are you?\"))\n",
    "# print(llm(\"what is AI?\"))\n",
    "# print(llm(\"Can you resume your previus answer?\")) #now memory work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't have feelings like humans do, so I can't say that I'm feeling any particular way. However, I'm here to help answer your questions and assist with tasks to the best of my abilities. How may I be of service today?\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Hello, how are you?\")\n",
    "# llm.generate(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial intelligence (AI) refers to the ability of machines or computer programs to mimic intelligent human behavior, such as learning, problem-solving, decision-making, perception, and language understanding. It involves the use of algorithms, data, and hardware to create systems that can perform tasks that typically require human intelligence, such as recognizing patterns, making predictions, and adapting to new situations.\n",
      "\n",
      "There are many different types of AI, including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as facial recognition, language translation, or playing chess. Narrow AI is trained on a specific dataset and is not capable of general reasoning or decision-making.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can. General AI has the ability to reason, learn, and improve its performance over time. It is still in development and has not yet been achieved.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence could potentially solve complex problems that are currently unsolvable, but it also raises concerns about safety and control.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can, and is considered a long-term goal for AI researchers. AGI would require a system that can learn, understand, and apply knowledge across a wide range of tasks and domains.\n",
      "5. Cognitive computing: This type of AI is designed to simulate human thought processes and mimic the way humans make decisions. Cognitive computing systems use machine learning, natural language processing, and other techniques to analyze large amounts of data and provide insights and recommendations.\n",
      "\n",
      "AI has many applications in various industries, including healthcare, finance, transportation, education, and entertainment. It has the potential to revolutionize the way we live and work, but also raises ethical and societal concerns related to privacy, bias, job displacement, and accountability.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"what is AI?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, my previous answer was:\n",
      "\n",
      "\"I apologize for the confusion. It seems that there is a problem with the prompt you provided. The prompt is not clear or specific enough to provide a useful response. Can you please rephrase or provide more context so I can better understand what you are asking and provide a helpful answer?\"\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Can you resume your previus answer?\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from typing import Optional, List, Dict, Mapping, Any\n",
    "import langchain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# 启动llm的缓存\n",
    "langchain.llm_cache = InMemoryCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLM(LLM):\n",
    "    history_data: Optional[List] = []\n",
    "    chatbot : Optional[hugchat.ChatBot] = None\n",
    "    conversation : Optional[str] = \"\"\n",
    "    email : Optional[str]\n",
    "    psw : Optional[str]\n",
    "    # 模型服务url\n",
    "    url = \"http://127.0.0.1:8595/chat\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chatglm\"\n",
    "\n",
    "    def _construct_query(self, prompt: str) -> Dict:\n",
    "        \"\"\"构造请求体\n",
    "        \"\"\"\n",
    "        query = {\n",
    "            \"human_input\": prompt\n",
    "        }\n",
    "        return query\n",
    "\n",
    "    @classmethod\n",
    "    def _post(cls, url: str,\n",
    "        query: Dict) -> Any:\n",
    "        \"\"\"POST请求\n",
    "        \"\"\"\n",
    "        _headers = {\"Content_Type\": \"application/json\"}\n",
    "        with requests.session() as sess:\n",
    "            resp = sess.post(url, \n",
    "                json=query, \n",
    "                headers=_headers, \n",
    "                timeout=60)\n",
    "        return resp\n",
    "\n",
    "    \n",
    "    def _call(self, prompt: str, \n",
    "        stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"_call\n",
    "        \"\"\"\n",
    "        # construct query\n",
    "        query = self._construct_query(prompt=prompt)\n",
    "\n",
    "        # post\n",
    "        resp = self._post(url=self.url,\n",
    "            query=query)\n",
    "        \n",
    "        if resp.status_code == 200:\n",
    "            resp_json = resp.json()\n",
    "            predictions = resp_json[\"response\"]\n",
    "            return predictions\n",
    "        else:\n",
    "            return \"请求模型\" \n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\n",
    "        \"\"\"\n",
    "        _param_dict = {\n",
    "            \"url\": self.url\n",
    "        }\n",
    "        return _param_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
